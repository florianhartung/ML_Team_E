{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team E - Pixels plus domain knowledge to True Shower parameters with CNNs\n",
    "\n",
    "## Team Members\n",
    "\n",
    "* Bjarne Karsten (3559535)\n",
    "* Marek Freunscht (9604914)\n",
    "* Florian Hartung (6622800)\n",
    "\n",
    "## Data and Example Download\n",
    "[...]\n",
    "\n",
    "* an exemplary jupyter notebook and futher information is available from the git repo https://github.com/astrojarred/magic-ml-images\n",
    "    * You find more information there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TASK \n",
    "\n",
    "* Improve your understanding of the data \n",
    "* The focus of this teams is the setup of MLPs or CNNs and combining them with domain knowledge like the hillas parameters\n",
    "* Use the pixel information features starting \"clean_image\" and the features starting with \"hillas\" and \"stereo\"\n",
    "    * they contain information of all the pixels for a simulated particle\n",
    "        * \"clean_image\" contains the data after some some standard cleaning algorithms were applied. These data are used in the MAGIC analysis to derive the Hillas Parameters.\n",
    "        * \"hillas\" and \"stereo\" features are derived from the raw images by using the standard MAGIC analysis workflow. \n",
    "* Create a baseline using a simple ML algorithm to\n",
    "    * classify gammas and protons \n",
    "        * by combining both clean_image_\\* features\n",
    "        * by combining both clean_image_\\* features with \"hillas\" and/or \"stereo\" \n",
    "    * predict/infer the true shower parameters using a simple ML algorithm\n",
    "        * using clean_image\\_\\*\\_m1 and clean_image\\_\\*\\_m2 independently\n",
    "        * by combining both clean_image_\\* features with \"hillas\" and/or \"stereo\" \n",
    "* Use MLP and simple CNN networks following the same strategy as above to\n",
    "    * classify gammas and protons \n",
    "    * predict/infer true shower parameters using a simple ML algorithm\n",
    "    * Think about a method to reuse the existing CNN kernels instead of using a hexagonal approch\n",
    "* compare the different approaches \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "TODO documentation\n",
    "\n",
    "- present project structure\n",
    "    - **this notebook**\n",
    "    - `src/`\n",
    "        - `common/`: Common python modules for this notebook and other modules\n",
    "            - `__init__.py`: Lists of grouped feature names like `FEATURES_HILLAS`, `FEATURES_STEREO` or `FEATURES_IMAGE_M1`\n",
    "            - `data_loading.py`: TODO\n",
    "            - `preprocessing.py`: TODO\n",
    "            - `HexaToParallelogramm.py`: TODO\n",
    "            - `batch.py`: TODO maybe remove this module?\n",
    "        - `classification_gammas_protons/`: TODO\n",
    "        - `infer_true_shower_parameters/`: TODO\n",
    "    - `magic-ml-images/`: TODO\n",
    "    - `data/`: Parquet files\n",
    "\n",
    "- present structure of this notebook + instructions how to evaluate, train, save and load individual models\n",
    "    - notebook initialization\n",
    "    - common dataloading for all models used\n",
    "    - classification models\n",
    "        - preprocess data for classification\n",
    "        - classification baseline #1\n",
    "        - classification baseline #2\n",
    "        - classification baseline #3\n",
    "        - advanced classification model #1\n",
    "        - advanced classification model #2\n",
    "        - advanced classification model #3\n",
    "    - regression models\n",
    "        - preprocess data for regression\n",
    "        - regression baseline #1\n",
    "        - regression baseline #2\n",
    "        - regression baseline #3\n",
    "        - advanced regression model #1\n",
    "        - advanced regression model #2\n",
    "        - advanced regression model #3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook\n",
    "First we load the `magicdl` module, which resides in `./magic-ml-images` as a git submodule (<https://github.com/astrojarred/magic-ml-images>).\n",
    "\n",
    "Also we set some global configuration variables and select an appropriate torch device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch device 'cuda:0'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Load magicdl module\n",
    "module_path = str(Path.cwd() / \"magic-ml-images\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import magicdl\n",
    "\n",
    "# The parent directory of all dataset parquet files\n",
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "\n",
    "# The parent directory of where trained models are saved to and loaded from\n",
    "TRAINED_MODELS_DIR = Path.cwd() / \"trained_models\"\n",
    "TRAINED_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Whether models trained from this notebook are saved automatically\n",
    "SAVE_MODELS_AFTER_TRAINING = True\n",
    "\n",
    "# Select torch device globally. This is later passed to all train/evaluate functions\n",
    "torch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using torch device '{torch_device}'\")\n",
    "\n",
    "# The seed used for all RNG in data preprocessing and models.\n",
    "# IMPORTANT: When this seed is changed, one must not load any models trained\n",
    "#            and saved with a previous different seed. Doing so would violate\n",
    "#            the ML process, because the previous training/validation data\n",
    "#            could now end up in the test dataset.\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading\n",
    "We have defined a module for loading both proton and gamma datasets.\n",
    "Our dataloading also flattens the image columns, so that every image column is converted to 1039 (number of pixels) individual columns each with a scalar value.\n",
    "This should not have any impact on performance or memory footprint (TODO Why?), but it allows models to use images as features more easily.\n",
    "\n",
    "In this step we also add class labels for both datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common import *\n",
    "from src.common.data_loading import load_dataset_and_flatten_images\n",
    "\n",
    "# We choose to only load relevant images and features to save memory\n",
    "FEATURES_TO_LOAD = FEATURES_HILLAS + FEATURES_STEREO + FEATURES_TRUE_SHOWER\n",
    "IMAGES_TO_LOAD = [\"clean_image_m1\", \"clean_image_m2\"]\n",
    "\n",
    "# Load the datasets\n",
    "data_gammas_raw = load_dataset_and_flatten_images(\n",
    "    DATA_DIR / \"magic-gammas-new-4.parquet\", IMAGES_TO_LOAD, FEATURES_TO_LOAD\n",
    ")\n",
    "data_protons_raw = load_dataset_and_flatten_images(\n",
    "    DATA_DIR / \"magic-protons.parquet\", IMAGES_TO_LOAD, FEATURES_TO_LOAD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Data preprocessing for classification models\n",
    "*Note: this cell need to be run only for classification models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.preprocessing import preprocess\n",
    "import pandas as pd\n",
    "\n",
    "# Assign binary class labels\n",
    "data_gammas_raw[\"class\"] = 0.0\n",
    "data_protons_raw[\"class\"] = 1.0\n",
    "\n",
    "# Concat both datasets. There is not need to shuffle, as that will be done by torch Dataloaders automatically\n",
    "data_classification = pd.concat([data_gammas_raw, data_protons_raw])\n",
    "\n",
    "normalize_params = (\n",
    "    FEATURES_HILLAS\n",
    "    + FEATURES_STEREO\n",
    "    + FEATURES_TRUE_SHOWER\n",
    "    + FEATURES_CLEAN_IMAGE_M1\n",
    "    + FEATURES_CLEAN_IMAGE_M2\n",
    ")\n",
    "\n",
    "data_classification_train, data_classification_validation, data_classification_test = (\n",
    "    preprocess(\n",
    "        data_classification,\n",
    "        normalize_params=normalize_params,\n",
    "        stratify_column_name=\"class\",\n",
    "        train_portion=0.7,\n",
    "        validation_portion=0.2,\n",
    "        seed=SEED,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Classification with CNN with Hillas\n",
    "This model uses a CNN combined with a fully-connected neural network to classify protons and gamma particles.\n",
    "It feeds both cleaned images (`clean_image_m1` & `clean_image_m2`) into a CNN, then concatenates its output with the hillas parameters and feeds those into the deep neural network, which then in turn outputs a single scalar value.\n",
    "\n",
    "The final layer does not use a sigmoid function itself, as this is not recommended for binary classification (TODO source?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Train Loss: 0.44480210 Acc: 0.79014318 | Val Loss: 0.41117360 Val Acc: 0.81838717 | Epoch took 161.62s\n",
      "Epoch 2/2 - Train Loss: 0.39031975 Acc: 0.82115025 | Val Loss: 0.37634924 Val Acc: 0.83442512 | Epoch took 160.07s\n"
     ]
    }
   ],
   "source": [
    "import src.classification_gammas_protons.cnn as classification_cnn\n",
    "\n",
    "TRAIN_THIS_MODEL = True\n",
    "THIS_MODEL_NAME = \"classification_cnn_with_hillas\"\n",
    "\n",
    "trained_model_path = TRAINED_MODELS_DIR / (THIS_MODEL_NAME + \".pth\")\n",
    "\n",
    "model = None\n",
    "if TRAIN_THIS_MODEL:\n",
    "    model = classification_cnn.train(\n",
    "        data_classification_train,\n",
    "        data_classification_validation,\n",
    "        image_features=[FEATURES_CLEAN_IMAGE_M1, FEATURES_CLEAN_IMAGE_M2],\n",
    "        additional_features=FEATURES_HILLAS,\n",
    "        class_feature=\"class\",\n",
    "        device=torch_device,\n",
    "        epochs=2,\n",
    "    )\n",
    "\n",
    "    if SAVE_MODELS_AFTER_TRAINING:\n",
    "        classification_cnn.save(model, trained_model_path)\n",
    "else:\n",
    "    model = classification_cnn.load(trained_model_path, additional_features=FEATURES_HILLAS)\n",
    "\n",
    "# TODO: Evaluate model on test data, maybe also show training/validation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing for regression models\n",
    "*Note: this cell only need to be run for all regression models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.preprocessing import preprocess\n",
    "import pandas as pd\n",
    "\n",
    "normalize_params = (\n",
    "    PARAMS_HILLAS\n",
    "    + PARAMS_STEREO\n",
    "    + PARAMS_TRUE_SHOWER\n",
    "    + PARAMS_CLEAN_IMAGE_M1\n",
    "    + PARAMS_CLEAN_IMAGE_M2\n",
    ")\n",
    "\n",
    "(\n",
    "    data_regression_gammas_train,\n",
    "    data_regression_gammas_validation,\n",
    "    data_regression_gammas_test,\n",
    ") = preprocess(\n",
    "    data_gammas_raw,\n",
    "    normalize_params=normalize_params,\n",
    "    train_portion=0.7,\n",
    "    validation_portion=0.2,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "(\n",
    "    data_regression_protons_train,\n",
    "    data_regression_protons_validation,\n",
    "    data_regression_protons_test,\n",
    ") = preprocess(\n",
    "    data_protons_raw,\n",
    "    normalize_params=normalize_params,\n",
    "    train_portion=0.7,\n",
    "    validation_portion=0.2,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
