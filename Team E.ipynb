{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team E - Pixels plus domain knowledge to True Shower parameters with CNNs\n",
    "\n",
    "## Team Members\n",
    "\n",
    "* Bjarne Karsten (3559535)\n",
    "* Marek Freunscht (9604914)\n",
    "* Florian Hartung (6622800)\n",
    "\n",
    "## Data and Example Download\n",
    "[...]\n",
    "\n",
    "* an exemplary jupyter notebook and futher information is available from the git repo https://github.com/astrojarred/magic-ml-images\n",
    "    * You find more information there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TASK \n",
    "\n",
    "* Improve your understanding of the data \n",
    "* The focus of this teams is the setup of MLPs or CNNs and combining them with domain knowledge like the hillas parameters\n",
    "* Use the pixel information features starting \"clean_image\" and the features starting with \"hillas\" and \"stereo\"\n",
    "    * they contain information of all the pixels for a simulated particle\n",
    "        * \"clean_image\" contains the data after some some standard cleaning algorithms were applied. These data are used in the MAGIC analysis to derive the Hillas Parameters.\n",
    "        * \"hillas\" and \"stereo\" features are derived from the raw images by using the standard MAGIC analysis workflow. \n",
    "* Create a baseline using a simple ML algorithm to\n",
    "    * classify gammas and protons \n",
    "        * by combining both clean_image_\\* features\n",
    "        * by combining both clean_image_\\* features with \"hillas\" and/or \"stereo\" \n",
    "    * predict/infer the true shower parameters using a simple ML algorithm\n",
    "        * using clean_image\\_\\*\\_m1 and clean_image\\_\\*\\_m2 independently\n",
    "        * by combining both clean_image_\\* features with \"hillas\" and/or \"stereo\" \n",
    "* Use MLP and simple CNN networks following the same strategy as above to\n",
    "    * classify gammas and protons \n",
    "    * predict/infer true shower parameters using a simple ML algorithm\n",
    "    * Think about a method to reuse the existing CNN kernels instead of using a hexagonal approch\n",
    "* compare the different approaches \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "TODO documentation\n",
    "\n",
    "- present project structure\n",
    "    - **this notebook**\n",
    "    - `src/`\n",
    "        - `common/`: Common python modules for this notebook and other modules\n",
    "            - `__init__.py`: Lists of grouped feature names like `FEATURES_HILLAS`, `FEATURES_STEREO` or `FEATURES_IMAGE_M1`\n",
    "            - `data_loading.py`: TODO\n",
    "            - `preprocessing.py`: TODO\n",
    "            - `HexaToParallelogramm.py`: TODO\n",
    "            - `batch.py`: TODO maybe remove this module?\n",
    "        - `classification_gammas_protons/`: TODO\n",
    "        - `infer_true_shower_parameters/`: TODO\n",
    "    - `magic-ml-images/`: TODO\n",
    "    - `data/`: Parquet files\n",
    "\n",
    "- present structure of this notebook + instructions how to evaluate, train, save and load individual models\n",
    "    - notebook initialization\n",
    "    - common dataloading for all models used\n",
    "    - classification models\n",
    "        - preprocess data for classification\n",
    "        - classification baseline #1\n",
    "        - classification baseline #2\n",
    "        - classification baseline #3\n",
    "        - advanced classification model #1\n",
    "        - advanced classification model #2\n",
    "        - advanced classification model #3\n",
    "    - regression models\n",
    "        - preprocess data for regression\n",
    "        - regression baseline #1\n",
    "        - regression baseline #2\n",
    "        - regression baseline #3\n",
    "        - advanced regression model #1\n",
    "        - advanced regression model #2\n",
    "        - advanced regression model #3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook\n",
    "First we load the `magicdl` module, which resides in `./magic-ml-images` as a git submodule (<https://github.com/astrojarred/magic-ml-images>).\n",
    "\n",
    "Also we set some global configuration variables and select an appropriate torch device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch device 'cuda:0'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Load magicdl module\n",
    "module_path = str(Path.cwd() / \"magic-ml-images\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import magicdl\n",
    "\n",
    "# The parent directory of all dataset parquet files\n",
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "\n",
    "# The parent directory of where trained models are saved to and loaded from\n",
    "TRAINED_MODELS_DIR = Path.cwd() / \"trained_models\"\n",
    "TRAINED_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Whether models trained from this notebook are saved automatically\n",
    "SAVE_MODELS_AFTER_TRAINING = True\n",
    "\n",
    "# Select torch device globally. This is later passed to all train/evaluate functions\n",
    "torch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using torch device '{torch_device}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading\n",
    "We have defined a module for loading both proton and gamma datasets.\n",
    "Our dataloading also flattens the image columns, so that every image column is converted to 1039 (number of pixels) individual columns each with a scalar value.\n",
    "This should not have any impact on performance or memory footprint (TODO Why?), but it allows models to use images as features more easily.\n",
    "\n",
    "In this step we also add class labels for both datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common import *\n",
    "from src.common.data_loading import load_dataset_and_flatten_images\n",
    "\n",
    "# We choose to only load relevant images and features to save memory\n",
    "FEATURES_TO_LOAD = FEATURES_HILLAS + FEATURES_STEREO + FEATURES_TRUE_SHOWER\n",
    "IMAGES_TO_LOAD = [\"clean_image_m1\", \"clean_image_m2\"]\n",
    "\n",
    "# Load the datasets\n",
    "data_gammas_raw = load_dataset_and_flatten_images(\n",
    "    DATA_DIR / \"magic-gammas-new-4.parquet\", IMAGES_TO_LOAD, FEATURES_TO_LOAD\n",
    ")\n",
    "data_protons_raw = load_dataset_and_flatten_images(\n",
    "    DATA_DIR / \"magic-protons.parquet\", IMAGES_TO_LOAD, FEATURES_TO_LOAD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Data preprocessing for classification models\n",
    "*Note: this cell need to be run only for classification models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.preprocessing import preprocess\n",
    "import pandas as pd\n",
    "\n",
    "# Assign binary class labels\n",
    "data_gammas_raw[\"class\"] = 0.0\n",
    "data_protons_raw[\"class\"] = 1.0\n",
    "\n",
    "# Concat both datasets. There is not need to shuffle, as that will be done by torch Dataloaders automatically\n",
    "data_classification = pd.concat([data_gammas_raw, data_protons_raw])\n",
    "\n",
    "normalize_params = (\n",
    "    FEATURES_HILLAS\n",
    "    + FEATURES_STEREO\n",
    "    + FEATURES_TRUE_SHOWER\n",
    "    + FEATURES_CLEAN_IMAGE_M1\n",
    "    + FEATURES_CLEAN_IMAGE_M2\n",
    ")\n",
    "\n",
    "data_classification_train, data_classification_validation, data_classification_test = (\n",
    "    preprocess(\n",
    "        data_classification,\n",
    "        normalize_params=normalize_params,\n",
    "        stratify_column_name=\"class\",\n",
    "        train_portion=0.7,\n",
    "        validation_portion=0.2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Classification with CNN with Hillas\n",
    "This model uses a CNN combined with a fully-connected neural network to classify protons and gamma particles.\n",
    "It feeds both cleaned images (`clean_image_m1` & `clean_image_m2`) into a CNN, then concatenates its output with the hillas parameters and feeds those into the deep neural network, which then in turn outputs a single scalar value.\n",
    "\n",
    "The final layer does not use a sigmoid function itself, as this is not recommended for binary classification (TODO source?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([('hillas_length_m1', 'hillas_width_m1', 'hillas_delta_m1', 'hillas_size_m1', 'hillas_cog_x_m1', 'hillas_cog_y_m1', 'hillas_sin_delta_m1', 'hillas_cos_delta_m1', 'hillas_length_m2', 'hillas_width_m2', 'hillas_delta_m2', 'hillas_size_m2', 'hillas_cog_x_m2', 'hillas_cog_y_m2', 'hillas_sin_delta_m2', 'hillas_cos_delta_m2')], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m model = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TRAIN_THIS_MODEL:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     model = \u001b[43mclassification_cnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_classification_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_classification_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mFEATURES_CLEAN_IMAGE_M1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFEATURES_CLEAN_IMAGE_M2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mFEATURES_HILLAS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclass\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SAVE_MODELS_AFTER_TRAINING:\n\u001b[32m     21\u001b[39m         classification_cnn.save(model, trained_model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/src/classification_gammas_protons/cnn.py:97\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_data, validation_data, image_features, additional_features, class_feature, device, epochs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Build torch datasets and dataloaders (this does not copy the data)\u001b[39;00m\n\u001b[32m     93\u001b[39m make_dataset = \u001b[38;5;28;01mlambda\u001b[39;00m data: ParticleClassificationDataset(\n\u001b[32m     94\u001b[39m     data, image_features, additional_features, class_feature, device\n\u001b[32m     95\u001b[39m )\n\u001b[32m     96\u001b[39m train_loader = DataLoader(\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     98\u001b[39m )\n\u001b[32m     99\u001b[39m validation_loader = DataLoader(\n\u001b[32m    100\u001b[39m     make_dataset(validation_data), batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    101\u001b[39m )\n\u001b[32m    103\u001b[39m model = ParticleCNNClassifier(\u001b[38;5;28mlen\u001b[39m(additional_features))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/src/classification_gammas_protons/cnn.py:93\u001b[39m, in \u001b[36mtrain.<locals>.<lambda>\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     90\u001b[39m batch_size = \u001b[32m32\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Build torch datasets and dataloaders (this does not copy the data)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m make_dataset = \u001b[38;5;28;01mlambda\u001b[39;00m data: \u001b[43mParticleClassificationDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m train_loader = DataLoader(\n\u001b[32m     97\u001b[39m     make_dataset(train_data), batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     98\u001b[39m )\n\u001b[32m     99\u001b[39m validation_loader = DataLoader(\n\u001b[32m    100\u001b[39m     make_dataset(validation_data), batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/src/classification_gammas_protons/cnn.py:22\u001b[39m, in \u001b[36mParticleClassificationDataset.__init__\u001b[39m\u001b[34m(self, df, image_features, additional_features, class_feature, device)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     15\u001b[39m     df,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     device,\n\u001b[32m     20\u001b[39m ):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m.images = [df[features].values \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m image_features]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mself\u001b[39m.features = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43madditional_features\u001b[49m\u001b[43m]\u001b[49m.values\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mself\u001b[39m.labels = df[[class_feature]].values\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = device\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/.devenv/state/venv/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/.devenv/state/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/.devenv/state/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index([('hillas_length_m1', 'hillas_width_m1', 'hillas_delta_m1', 'hillas_size_m1', 'hillas_cog_x_m1', 'hillas_cog_y_m1', 'hillas_sin_delta_m1', 'hillas_cos_delta_m1', 'hillas_length_m2', 'hillas_width_m2', 'hillas_delta_m2', 'hillas_size_m2', 'hillas_cog_x_m2', 'hillas_cog_y_m2', 'hillas_sin_delta_m2', 'hillas_cos_delta_m2')], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import src.classification_gammas_protons.cnn as classification_cnn\n",
    "\n",
    "TRAIN_THIS_MODEL = True\n",
    "THIS_MODEL_NAME = \"classification_cnn_with_hillas\"\n",
    "\n",
    "trained_model_path = TRAINED_MODELS_DIR / (THIS_MODEL_NAME + \".pth\")\n",
    "\n",
    "model = None\n",
    "if TRAIN_THIS_MODEL:\n",
    "    model = classification_cnn.train(\n",
    "        data_classification_train,\n",
    "        data_classification_validation,\n",
    "        image_features=[FEATURES_CLEAN_IMAGE_M1, FEATURES_CLEAN_IMAGE_M2],\n",
    "        additional_features=FEATURES_HILLAS,\n",
    "        class_feature=\"class\",\n",
    "        device=torch_device,\n",
    "        epochs=2,\n",
    "    )\n",
    "\n",
    "    if SAVE_MODELS_AFTER_TRAINING:\n",
    "        classification_cnn.save(model, trained_model_path)\n",
    "else:\n",
    "    model = classification_cnn.load(trained_model_path, additional_features=FEATURES_HILLAS)\n",
    "\n",
    "# TODO: Evaluate model on test data, maybe also show training/validation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing for regression models\n",
    "*Note: this cell only need to be run for all regression models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.preprocessing import preprocess\n",
    "import pandas as pd\n",
    "\n",
    "normalize_params = (\n",
    "    PARAMS_HILLAS\n",
    "    + PARAMS_STEREO\n",
    "    + PARAMS_TRUE_SHOWER\n",
    "    + PARAMS_CLEAN_IMAGE_M1\n",
    "    + PARAMS_CLEAN_IMAGE_M2\n",
    ")\n",
    "\n",
    "data_regression_gammas_train, data_regression_gammas_validation, data_regression_gammas_test = (\n",
    "    preprocess(\n",
    "        data_gammas_raw,\n",
    "        normalize_params=normalize_params,\n",
    "        train_portion=0.7,\n",
    "        validation_portion=0.2,\n",
    "    )\n",
    ")\n",
    "\n",
    "data_regression_protons_train, data_regression_protons_validation, data_regression_protons_test = (\n",
    "    preprocess(\n",
    "        data_protons_raw,\n",
    "        normalize_params=normalize_params,\n",
    "        train_portion=0.7,\n",
    "        validation_portion=0.2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
