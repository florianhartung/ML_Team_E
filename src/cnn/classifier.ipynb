{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include project root in path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"./magic-ml-images\")\n",
    "sys.path.append(\"../../magic-ml-images\")\n",
    "\n",
    "# this increases performance?\n",
    "import torch\n",
    "torch.multiprocessing.set_start_method(\"spawn\")\n",
    "\n",
    "# Load data\n",
    "from src.common import preprocessing, datasets\n",
    "import pandas as pd\n",
    "\n",
    "gammas = datasets.read_gammas()\n",
    "protons = datasets.read_protons()\n",
    "\n",
    "gammas[\"class\"] = 1.0\n",
    "protons[\"class\"] = 0.0\n",
    "\n",
    "data = pd.concat([gammas, protons])\n",
    "\n",
    "# Preprocess data\n",
    "from src.common.preprocessing import preprocess\n",
    "from src.common import (\n",
    "    PARAMS_HILLAS,\n",
    "    PARAMS_TRUE_SHOWER,\n",
    "    PARAMS_IMAGE_M1,\n",
    "    PARAMS_IMAGE_M2,\n",
    "    PARAMS_CLEAN_IMAGE_M1,\n",
    "    PARAMS_CLEAN_IMAGE_M2,\n",
    ")\n",
    "\n",
    "train, validation, test = preprocess(\n",
    "    data,\n",
    "    normalize_params=PARAMS_HILLAS + PARAMS_CLEAN_IMAGE_M1 + PARAMS_CLEAN_IMAGE_M2,\n",
    "    stratify_column_name=\"class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want to use torch, we now build torch datasets from previous Pandas dataframes\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ParticleClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "    ):\n",
    "        self.images = [\n",
    "            df[PARAMS_CLEAN_IMAGE_M1].values,\n",
    "            df[PARAMS_CLEAN_IMAGE_M2].values,\n",
    "        ]\n",
    "        self.features = df[PARAMS_HILLAS].values\n",
    "        self.labels = df[[\"class\"]].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = torch.stack(list(map(lambda image: torch.tensor(image[idx], dtype=torch.float), self.images))).to(device)\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float)\n",
    "        label = torch.squeeze(torch.tensor(self.labels[idx], dtype=torch.float))\n",
    "        return images, features, label\n",
    "\n",
    "\n",
    "train_dataset = ParticleClassificationDataset(train)\n",
    "validation_dataset = ParticleClassificationDataset(validation)\n",
    "test_dataset = ParticleClassificationDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Select a supported torch device\n",
    "import torch\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cuda:0\"\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.common.HexaToParallelogram import HexaToParallelogram\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(2, 8, 5), # 2, 6, 5\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(8, 16, 5),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.model(images)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParticleCNNClassifier(nn.Module):\n",
    "    def __init__(self, num_additional_parameters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hex2par = HexaToParallelogram(2)\n",
    "        self.cnn = CNN()\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6 + num_additional_parameters, 128) \n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, images, additional_parameters):\n",
    "        x = self.hex2par(images)\n",
    "\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = torch.cat((x, additional_parameters), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "def train_model(model, train_dataset, validation_dataset, optimizer, epochs=10, batch_size=32):\n",
    "    model.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        epoch_accum_loss = 0\n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        for images, additional_features, labels in train_loader:\n",
    "            images, additional_features, labels = images.to(device), additional_features.to(device), labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images, additional_features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_accum_loss += loss.item()\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        epoch_accum_loss /= len(train_loader)\n",
    "        epoch_accuracy = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, validation_loader, criterion, device)\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_accum_loss:.4f} Acc: {epoch_accuracy:.4f} | Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f} | Epoch took {epoch_end - epoch_start:.2f}s\")\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device) -> (float, float):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, additional_features, labels in dataloader:\n",
    "            images, additional_features, labels = images.to(device), additional_features.to(device), labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images, additional_features)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return loss / len(dataloader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.4045 Acc: 0.8182 | Val Loss: 0.3658 Val Acc: 0.8478 | Epoch took 75.33s\n",
      "Epoch 2/30 - Train Loss: 0.3649 Acc: 0.8421 | Val Loss: 0.3571 Val Acc: 0.8472 | Epoch took 75.53s\n",
      "Epoch 3/30 - Train Loss: 0.3590 Acc: 0.8441 | Val Loss: 0.3671 Val Acc: 0.8419 | Epoch took 75.97s\n",
      "Epoch 4/30 - Train Loss: 0.7087 Acc: 0.6933 | Val Loss: 0.6466 Val Acc: 0.6513 | Epoch took 75.09s\n",
      "Epoch 5/30 - Train Loss: 0.6473 Acc: 0.6513 | Val Loss: 0.6469 Val Acc: 0.6513 | Epoch took 76.89s\n",
      "Epoch 6/30 - Train Loss: 0.5319 Acc: 0.7418 | Val Loss: 0.4340 Val Acc: 0.7997 | Epoch took 74.26s\n",
      "Epoch 7/30 - Train Loss: 0.3765 Acc: 0.8355 | Val Loss: 0.3775 Val Acc: 0.8375 | Epoch took 76.27s\n",
      "Epoch 8/30 - Train Loss: 0.3607 Acc: 0.8436 | Val Loss: 0.3534 Val Acc: 0.8461 | Epoch took 75.76s\n",
      "Epoch 9/30 - Train Loss: 0.3910 Acc: 0.8281 | Val Loss: 0.3970 Val Acc: 0.8256 | Epoch took 75.44s\n",
      "Epoch 10/30 - Train Loss: 0.3649 Acc: 0.8423 | Val Loss: 0.3525 Val Acc: 0.8485 | Epoch took 74.95s\n",
      "Epoch 11/30 - Train Loss: 0.3530 Acc: 0.8480 | Val Loss: 0.3551 Val Acc: 0.8498 | Epoch took 75.55s\n",
      "Epoch 12/30 - Train Loss: 0.3521 Acc: 0.8486 | Val Loss: 0.3506 Val Acc: 0.8527 | Epoch took 74.76s\n",
      "Epoch 13/30 - Train Loss: 0.3883 Acc: 0.8305 | Val Loss: 0.5247 Val Acc: 0.7657 | Epoch took 75.33s\n",
      "Epoch 14/30 - Train Loss: 0.6213 Acc: 0.6728 | Val Loss: 0.6466 Val Acc: 0.6513 | Epoch took 76.08s\n",
      "Epoch 15/30 - Train Loss: 0.4677 Acc: 0.7795 | Val Loss: 0.3857 Val Acc: 0.8282 | Epoch took 75.22s\n",
      "Epoch 16/30 - Train Loss: 0.3638 Acc: 0.8435 | Val Loss: 0.3775 Val Acc: 0.8331 | Epoch took 75.09s\n",
      "Epoch 17/30 - Train Loss: 0.3794 Acc: 0.8321 | Val Loss: 0.6468 Val Acc: 0.6513 | Epoch took 75.21s\n",
      "Epoch 18/30 - Train Loss: 0.3978 Acc: 0.8208 | Val Loss: 0.3556 Val Acc: 0.8448 | Epoch took 76.23s\n",
      "Epoch 19/30 - Train Loss: 0.4082 Acc: 0.8216 | Val Loss: 0.3544 Val Acc: 0.8448 | Epoch took 75.61s\n",
      "Epoch 20/30 - Train Loss: 0.3583 Acc: 0.8448 | Val Loss: 0.3522 Val Acc: 0.8486 | Epoch took 74.81s\n",
      "Epoch 21/30 - Train Loss: 0.3600 Acc: 0.8433 | Val Loss: 0.4139 Val Acc: 0.8233 | Epoch took 75.60s\n",
      "Epoch 22/30 - Train Loss: 0.3890 Acc: 0.8324 | Val Loss: 0.5417 Val Acc: 0.7637 | Epoch took 74.74s\n",
      "Epoch 23/30 - Train Loss: 0.3689 Acc: 0.8417 | Val Loss: 0.9004 Val Acc: 0.4558 | Epoch took 74.88s\n",
      "Epoch 24/30 - Train Loss: 0.3584 Acc: 0.8435 | Val Loss: 0.3462 Val Acc: 0.8532 | Epoch took 76.93s\n",
      "Epoch 25/30 - Train Loss: 0.3500 Acc: 0.8484 | Val Loss: 0.3546 Val Acc: 0.8493 | Epoch took 75.64s\n",
      "Epoch 26/30 - Train Loss: 0.3535 Acc: 0.8471 | Val Loss: 0.3420 Val Acc: 0.8557 | Epoch took 76.21s\n",
      "Epoch 27/30 - Train Loss: 0.3819 Acc: 0.8322 | Val Loss: 0.6468 Val Acc: 0.6517 | Epoch took 76.10s\n",
      "Epoch 28/30 - Train Loss: 0.4204 Acc: 0.8095 | Val Loss: 0.3490 Val Acc: 0.8481 | Epoch took 75.38s\n",
      "Epoch 29/30 - Train Loss: 0.3499 Acc: 0.8486 | Val Loss: 0.3446 Val Acc: 0.8540 | Epoch took 76.05s\n",
      "Epoch 30/30 - Train Loss: 0.3610 Acc: 0.8439 | Val Loss: 0.3578 Val Acc: 0.8466 | Epoch took 76.30s\n"
     ]
    }
   ],
   "source": [
    "model = ParticleCNNClassifier(len(PARAMS_HILLAS))\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.0, weight_decay=0.001)\n",
    "# lr=0.001, momentum=0.0, weight_decay=0.001\n",
    "# optimizer = optim.Adam(lr=lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "\n",
    "train_model(model, train_dataset, validation_dataset, optimizer, epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
