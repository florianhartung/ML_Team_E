{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include project root in path\n",
    "# import sys\n",
    "# sys.path.append(\"../..\")\n",
    "\n",
    "# Load data\n",
    "from src.common import preprocessing, datasets\n",
    "data = datasets.read_gammas()\n",
    "\n",
    "# Preprocess data\n",
    "from src.common.preprocessing import preprocess\n",
    "from src.common import PARAMS_HILLAS, PARAMS_TRUE_SHOWER, PARAMS_CLEAN_IMAGE_M1, PARAMS_CLEAN_IMAGE_M2\n",
    "train, validation, test = preprocess(data, normalize_params=PARAMS_HILLAS + PARAMS_TRUE_SHOWER + PARAMS_CLEAN_IMAGE_M1 + PARAMS_CLEAN_IMAGE_M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.common.HexaToParallelogram import HexaToParallelogram\n",
    "\n",
    "\n",
    "# class Combine(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         self.cnn:nn.Module = None\n",
    "#         self.input2size\n",
    "\n",
    "#         self.fc1\n",
    "\n",
    "#     def forward(self, images, linear):\n",
    "#         X = fc1(torch.cat(self.cnn(images), linear))\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 6, 5)  # 22x22\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 18x18\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # 9x9\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.pool(F.relu(self.conv1(images)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParticleCNNRegressor(nn.Module):\n",
    "    def __init__(self, num_additional_parameters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hex2par = HexaToParallelogram()\n",
    "        self.cnn = CNN()\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6 + num_additional_parameters, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 8)\n",
    "\n",
    "    def forward(self, images, additional_parameters):\n",
    "        x = torch.stack([self.hex2par(images[0]), self.hex2par(images[1])])\n",
    "\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x)\n",
    "\n",
    "        x = torch.cat((x, additional_parameters))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.492\n",
      "[1,   400] loss: 0.172\n",
      "[1,   600] loss: 1.928\n",
      "[1,   800] loss: -0.154\n",
      "[1,  1000] loss: 0.062\n",
      "[1,  1200] loss: 0.311\n",
      "[1,  1400] loss: 0.602\n",
      "[1,  1600] loss: 1.363\n",
      "[1,  1800] loss: 0.332\n",
      "[1,  2000] loss: 0.139\n",
      "[1,  2200] loss: -0.150\n",
      "[1,  2400] loss: -0.650\n",
      "[1,  2600] loss: -0.771\n",
      "[1,  2800] loss: -0.025\n",
      "[1,  3000] loss: -0.203\n",
      "[1,  3200] loss: 1.072\n",
      "[1,  3400] loss: -0.258\n",
      "[1,  3600] loss: -0.302\n",
      "[1,  3800] loss: -0.562\n",
      "[1,  4000] loss: -0.193\n",
      "[1,  4200] loss: 0.769\n",
      "[1,  4400] loss: -0.437\n",
      "[1,  4600] loss: 0.274\n",
      "[1,  4800] loss: -0.537\n",
      "[1,  5000] loss: 0.531\n",
      "[1,  5200] loss: 0.882\n",
      "[1,  5400] loss: 0.650\n",
      "[1,  5600] loss: -0.316\n",
      "[1,  5800] loss: 0.787\n",
      "[1,  6000] loss: -0.116\n",
      "[1,  6200] loss: 0.533\n",
      "[1,  6400] loss: -0.908\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m additional_properties = torch.tensor(d[PARAMS_HILLAS].values, dtype=torch.float)\n\u001b[32m     18\u001b[39m target_values = torch.tensor(d[PARAMS_TRUE_SHOWER].values, dtype=torch.float)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m outputs = \u001b[43mregressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_properties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m loss = criterion(outputs, target_values)\n\u001b[32m     23\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/.devenv/state/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/.devenv/state/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mParticleCNNRegressor.forward\u001b[39m\u001b[34m(self, images, additional_parameters)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, additional_parameters):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     x = torch.stack([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhex2par\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.hex2par(images[\u001b[32m1\u001b[39m])])\n\u001b[32m     46\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.cnn(x)\n\u001b[32m     47\u001b[39m     x = torch.flatten(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/.devenv/state/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/.devenv/state/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/florianhartung/ML_Team_E/src/common/HexaToParallelogram.py:21\u001b[39m, in \u001b[36mHexaToParallelogram.forward\u001b[39m\u001b[34m(self, hexa)\u001b[39m\n\u001b[32m     19\u001b[39m         key = (q-q_max, r-r_max)\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lt.keys():\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m             tensor[q][r] = hexa[\u001b[38;5;28mself\u001b[39m._lt[key]]\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "regressor = ParticleCNNRegressor(len(PARAMS_HILLAS))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(regressor.parameters(recurse=True), lr=0.00001, momentum=0.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        d = train.iloc[i]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        clean_image_m1 = torch.tensor(d[PARAMS_CLEAN_IMAGE_M1].values, dtype=torch.float)\n",
    "        clean_image_m2 = torch.tensor(d[PARAMS_CLEAN_IMAGE_M2].values, dtype=torch.float)\n",
    "        images = torch.stack([clean_image_m1, clean_image_m2])\n",
    "        additional_properties = torch.tensor(d[PARAMS_HILLAS].values, dtype=torch.float)\n",
    "        target_values = torch.tensor(d[PARAMS_TRUE_SHOWER].values, dtype=torch.float)\n",
    "\n",
    "\n",
    "        outputs = regressor(images, additional_properties)\n",
    "        loss = criterion(outputs, target_values)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
